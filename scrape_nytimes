#!/usr/bin/env python3

from urllib.parse import urljoin
import requests
from lxml import etree
import time
from random import choice
import re

'''Issues:
1. We need a better criteria for ending the crawling
2. We need to add timers to the crawling intervals: Done but can be improved
3. Throw in a bit of randomness into the websites to scrape
4. Throw in a bit of randomness with the waiting time: Done but can be improved
5. We need a collection of User-Agents :::: Done!
'''



urls=set()
visited=[]

def get_content(url):
    '''Opens content of browser'''
    #using Changing the User Agent
    headers= {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\
     (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.3'}
    try:
        print('website to scrape is : '+url)
        response = requests.get(url,headers=headers).text
        print('got contents')
    except UnboundLocalError as e:
            print('invalid url: {0}'.format(url))
            print(e)
    else:
        return response

def stack_urls(new_urls):
    """Stacks the urls"""
    urls.update(new_urls)
    print('I have stacked')

def track(url):
    '''Keeps track of urls'''
    visited.append(url)
    print('tracking...')

def get_links(baseurl):
    '''Get the links from the webpage'''
    #get contents
    contents=get_content(baseurl)
    #build html tree
    if 'html' in contents.lower():
        tree= etree.HTML(contents)
    else:
        tree=etree.XML(contents)

    #grab the links
    links =[link.get('href') for link in tree.xpath('//a[@href]')]

    for link in links:
        if re.compile(r'[#]').match(link) != None:
            links.remove(link)
        elif '@' in link:
            links.remove(link)
        elif (len(link)==0) or (len(link)==1):
            links.remove(link)
        elif link.lower() == 'javascript':
            links.remove(link)
        elif re.compile(r'.*\/index.html').search(link) !=None:
            links.remove(link)

    links=create_url(baseurl,links)
    #stacks the links
    stack_urls([link for link in links])
    print('got links')

def create_url(baseurl,urls):

    '''Construct appropriate urls '''
    # Making sure the base url has trailing / character
    if re.compile('\/$').match(baseurl) == None :
        baseurl= baseurl + '/'
    else:
        pass

    # parse links
    for link in urls:
        if link == '/':
            continue

        if link[:1]== '/' and link[1] != '/':
            print(link + ': /')
            yield urljoin(baseurl,link[1:])

        elif re.compile(r"\.\.\/").match(link) != None:
            print(link+ ':  ../')
            yield urljoin(baseurl,link[3:])

        elif re.compile(r"\.\/").match(link) != None:
            print(link+ ':  ./')
            yield urljoin(baseurl, link[2:])

        elif link[:2] == '//' :
            print(link+ ':  //')
            yield link


        else:
            yield link




def scrapper(baseurl,*timers):
    '''This is a crawler
    timers: list of times for waiting
     '''


    totalurl= 0
    urls.update([baseurl])

    #crawls the web
    while True:
        #new website to craw
        oldurl=urls.pop()
        while True:
            #Checks if url hasn't been crawled already
            if not oldurl in visited:
                get_links(oldurl)
                break
            else:
                if len(urls)==0:
                    break
                else:
                    oldurl=urls.pop()
        waittime=choice(timers)
        time.sleep(waittime)
        print('Waited for '+ str(waittime) + ' seconds before scrapping next site')
        track(oldurl)
        totalurl+=len(urls)   # keeps track of the size of stack
        print('total links {0}'.format(totalurl))
        #condition for ending the crawing
        if totalurl==0 or totalurl>=6000:
            return







if __name__  == '__main__':

    url= "http://nytimes.com"
    # url='http://www.cnn.com'
    scrapper(url,5,6,7,1,3)
    print('total websites visited: {0}'.format(len(visited)))

    #base='http://some'
    # urls=['//cnn.com','../cnn','www.ug.edu.gh','/more/new','./hey']



#response=requests.get(url).content

#parser=etree.HTMLParser(encoding='utf-8')

#tree=etree.fromstring(reponse,parser)

#print([link.get('href') for link in tree.xpath('//a[@href]')])

#soup=BeautifulSoup(response,'lxml')

#print([link.string for link in soup.find_all('a',href=True)])
